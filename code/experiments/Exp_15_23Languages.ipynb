{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"     \\nwith open('subset1/eng.txt','rt') as f:\\n    text1 = f.read().lower()\\nwith open('subset1/frn.txt','rt') as f:\\n    text2 = f.read().lower()\\nwith open('subset/ger.txt','rt') as f:\\n    text3 = f.read().lower()\\nprint('corpus length:', len(text1))\\nprint('corpus length:', len(text2))\\nprint('corpus length:', len(text3))\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "text = []\n",
    "for i,l in enumerate(languages):\n",
    "    with open('subset/'+ l + '.txt','rt') as f:\n",
    "        doc = f.read().lower()\n",
    "    text.append(doc)\n",
    "'''     \n",
    "with open('subset1/eng.txt','rt') as f:\n",
    "    text1 = f.read().lower()\n",
    "with open('subset1/frn.txt','rt') as f:\n",
    "    text2 = f.read().lower()\n",
    "with open('subset/ger.txt','rt') as f:\n",
    "    text3 = f.read().lower()\n",
    "print('corpus length:', len(text1))\n",
    "print('corpus length:', len(text2))\n",
    "print('corpus length:', len(text3))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 131\n"
     ]
    }
   ],
   "source": [
    "text1 = text[0]\n",
    "text2 = text[1]\n",
    "text3 = text[2]\n",
    "text4 = text[3]\n",
    "text5 = text[4]\n",
    "\n",
    "text6 = text[5]\n",
    "text7 = text[6]\n",
    "text8 = text[7]\n",
    "text9 = text[8]\n",
    "text10 = text[9]\n",
    "text11 = text[10]\n",
    "text12 = text[11]\n",
    "text13 = text[12]\n",
    "text14 = text[13]\n",
    "text15 = text[14]\n",
    "text16 = text[15]\n",
    "text17 = text[16]\n",
    "text18 = text[17]\n",
    "text19 = text[18]\n",
    "text20 = text[19]\n",
    "text21 = text[20]\n",
    "text22 = text[21]\n",
    "text23 = text[22]\n",
    "chars = sorted(list(set(text[0] + text[1] +text[2] +text[3]+ text[4] + text[5] +text[6] + text[7]+ text[8] + text[9] \n",
    "                        +text[10] +text[11] + text[12] +text[13] +text[14] + text[15] +text[16] +text[17] + text[18] \n",
    "                        +text[19] +text[20] + text[21] + text[22])))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#generating language model\n",
    "def build_model(text, chars, char_indices):\n",
    "    ##training set prepare\n",
    "    maxlen= 40\n",
    "    step = 1\n",
    "    sentences_tr = []\n",
    "    next_chars_tr = []\n",
    "    for i in range(0, int(0.8*len(text)) - maxlen, step):\n",
    "        sentences_tr.append(text[i: i + maxlen])\n",
    "        next_chars_tr.append(text[i + maxlen])\n",
    "\n",
    "    print('nb sequences:', len(sentences_tr))\n",
    "    ##test set prepare\n",
    "    sentences_test = []\n",
    "    string_test= []\n",
    "    step_test=20\n",
    "    for i in range(int(0.8*len(text)) - maxlen,len(text) - maxlen, step_test):\n",
    "        sentences_test.append(text[i: i + maxlen])\n",
    "        string_test.append(text[i+maxlen:i+maxlen+50])\n",
    "    print('nb sequences:', len(sentences_test))\n",
    "\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(sentences_tr), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences_tr), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences_tr):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars_tr[i]]] = 1\n",
    "\n",
    "\n",
    "    # build the model: a single LSTM\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return X, y, sentences_test, string_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 8556\n",
      "nb sequences: 108\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9857\n",
      "nb sequences: 124\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9760\n",
      "nb sequences: 123\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8936\n",
      "nb sequences: 113\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9850\n",
      "nb sequences: 124\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10270\n",
      "nb sequences: 129\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 18195\n",
      "nb sequences: 228\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10589\n",
      "nb sequences: 133\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10244\n",
      "nb sequences: 129\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9935\n",
      "nb sequences: 125\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10852\n",
      "nb sequences: 137\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8808\n",
      "nb sequences: 111\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8012\n",
      "nb sequences: 101\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8056\n",
      "nb sequences: 102\n",
      "Vectorization...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "X_eng, y_eng, sentences_eng_test, string_eng_test, model1 = build_model(text1, chars, char_indices) \n",
    "X_fr, y_fr, sentences_fr_test, string_fr_test, model2 = build_model(text2, chars, char_indices) \n",
    "X_ger, y_ger, sentences_ger_test, string_ger_test, model3 = build_model(text3, chars, char_indices) \n",
    "X_czc, y_czc, sentences_czc_test, string_czc_test, model4 = build_model(text4, chars, char_indices) \n",
    "X_dns, y_dns, sentences_dns_test, string_dns_test, model5 = build_model(text5, chars, char_indices) \n",
    "X_dut, y_dut, sentences_dut_test, string_dut_test, model6 = build_model(text6, chars, char_indices) \n",
    "X_grk, y_grk, sentences_grk_test, string_grk_test, model7 = build_model(text7, chars, char_indices) \n",
    "\n",
    "X_hng, y_hng, sentences_hng_test, string_hng_test, model8 = build_model(text8, chars, char_indices) \n",
    "X_itn, y_itn, sentences_itn_test, string_itn_test, model9 = build_model(text9, chars, char_indices) \n",
    "X_jpn, y_jpn, sentences_jpn_test, string_jpn_test, model10 = build_model(text10, chars, char_indices) \n",
    "X_lat, y_lat, sentences_lat_test, string_lat_test, model11 = build_model(text11, chars, char_indices) \n",
    "X_lit, y_lit, sentences_lit_test, string_lit_test, model12 = build_model(text12, chars, char_indices) \n",
    "X_ltn, y_ltn, sentences_ltn_test, string_ltn_test, model13 = build_model(text13, chars, char_indices) \n",
    "X_ltn1, y_ltn1, sentences_ltn1_test, string_ltn1_test, model14 = build_model(text14, chars, char_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 10244\n",
      "nb sequences: 129\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9359\n",
      "nb sequences: 118\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9444\n",
      "nb sequences: 119\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9194\n",
      "nb sequences: 116\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10176\n",
      "nb sequences: 128\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 17442\n",
      "nb sequences: 219\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9788\n",
      "nb sequences: 123\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 16456\n",
      "nb sequences: 207\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8850\n",
      "nb sequences: 112\n",
      "Vectorization...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "X_lux, y_lux, sentences_lux_test, string_lux_test, model15 = build_model(text15, chars, char_indices) \n",
    "X_mls, y_mls, sentences_mls_test, string_mls_test, model16 = build_model(text16, chars, char_indices) \n",
    "X_por, y_por, sentences_por_test, string_por_test, model17 = build_model(text17, chars, char_indices) \n",
    "X_rmn1, y_rmn1, sentences_rmn1_test, string_rmn1_test, model18 = build_model(text18, chars, char_indices) \n",
    "X_rum, y_rum, sentences_rum_test, string_rum_test, model19 = build_model(text19, chars, char_indices) \n",
    "X_rus, y_rus, sentences_rus_test, string_rus_test, model20 = build_model(text20, chars, char_indices) \n",
    "X_spn, y_spn, sentences_spn_test, string_spn_test, model21 = build_model(text21, chars, char_indices)\n",
    "\n",
    "X_ukr, y_ukr, sentences_ukr_test, string_ukr_test, model22 = build_model(text22, chars, char_indices) \n",
    "X_yps, y_yps, sentences_yps_test, string_yps_test, model23 = build_model(text23, chars, char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(sentence, chars , char_indices ):\n",
    "    X = np.zeros((1, maxlen, len(chars)), dtype=np.bool)\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[0, t, char_indices[char]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_scores_for_model(seed,next_string, model,chars,char_indices):\n",
    "    sentence =seed\n",
    "    prob_char=0.0\n",
    "    x = np.zeros((5, maxlen, len(chars)))\n",
    "\n",
    "    x[0] = vectorize(seed,chars,char_indices)\n",
    "    x[1] = vectorize(seed[1:]+next_string[:1],chars,char_indices)\n",
    "    x[2] = vectorize(seed[2:]+next_string[:2],chars,char_indices)\n",
    "    x[3] = vectorize(seed[3:]+next_string[:3],chars,char_indices)\n",
    "    x[4] = vectorize(seed[4:]+next_string[:4],chars,char_indices)\n",
    "    probs_term = []\n",
    "    for i in range(5):\n",
    "        preds = model.predict(x[i].reshape(1,40,131), verbose=0)[0]\n",
    "        preds = np.log(preds)\n",
    "        probs_term.append(preds[char_indices[next_string[i]]])\n",
    "\n",
    "    return probs_term\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "#Create a test SET of all language sentences\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "sentences_test=[]\n",
    "string_test=[]\n",
    "test_size = 10\n",
    "sentences_test.append(sentences_fr_test[:test_size])\n",
    "string_test.append(string_fr_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_eng_test[:test_size])\n",
    "string_test.append(string_eng_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_ger_test[:test_size])\n",
    "string_test.append(string_ger_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_czc_test[:test_size])\n",
    "string_test.append(string_czc_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_dns_test[:test_size])\n",
    "string_test.append(string_dns_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_dut_test[:test_size])\n",
    "string_test.append(string_dut_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_grk_test[:test_size])\n",
    "string_test.append(string_grk_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_hng_test[:test_size])\n",
    "string_test.append(string_hng_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_itn_test[:test_size])\n",
    "string_test.append(string_itn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_jpn_test[:test_size])\n",
    "string_test.append(string_jpn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_lat_test[:test_size])\n",
    "string_test.append(string_lat_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_lit_test[:test_size])\n",
    "string_test.append(string_lit_test[:test_size])\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "sentences_test.append(sentences_ltn_test[:test_size])\n",
    "string_test.append(string_ltn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_ltn1_test[:test_size])\n",
    "string_test.append(string_ltn1_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_lux_test[:test_size])\n",
    "string_test.append(string_lux_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_mls_test[:test_size])\n",
    "string_test.append(string_mls_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_por_test[:test_size])\n",
    "string_test.append(string_por_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_rmn1_test[:test_size])\n",
    "string_test.append(string_rmn1_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_rum_test[:test_size])\n",
    "string_test.append(string_rum_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_rus_test[:test_size])\n",
    "string_test.append(string_rus_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_spn_test[:test_size])\n",
    "string_test.append(string_spn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_ukr_test[:test_size])\n",
    "string_test.append(string_ukr_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_yps_test[:test_size])\n",
    "string_test.append(string_yps_test[:test_size])\n",
    "\n",
    "print(len(sentences_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ent n\\xc3\\xa9s dans le mariage ou hors mariage, jouissen', 'iage ou hors mariage, jouissent de la m\\xc3\\xaame protec', ', jouissent de la m\\xc3\\xaame protection sociale. articl', '\\xaame protection sociale. article 26 1. toute person', \"le. article 26 1. toute personne a droit \\xc3\\xa0 l'\\xc3\\xa9du\", \"ute personne a droit \\xc3\\xa0 l'\\xc3\\xa9ducation. l'\\xc3\\xa9ducation\", \" \\xc3\\xa0 l'\\xc3\\xa9ducation. l'\\xc3\\xa9ducation doit \\xc3\\xaatre gratuite\", '\\xc3\\xa9ducation doit \\xc3\\xaatre gratuite, au moins en ce qui', \"e gratuite, au moins en ce qui concerne l'enseigne\", \" en ce qui concerne l'enseignement \\xc3\\xa9l\\xc3\\xa9mentaire e\"], ['n or out of wedlock, shall enjoy the same social p', ' shall enjoy the same social protection. article 2', 'e social protection. article 26 1. everyone has th', ' article 26 1. everyone has the right to education', 'one has the right to education. education shall be', ' education. education shall be free, at least in t', 'n shall be free, at least in the elementary and fu', 'least in the elementary and fundamental stages. el', 'ary and fundamental stages. elementary education s', 'stages. elementary education shall be compulsory. '], ['liche wie au\\xc3\\x9fereheliche, genie\\xc3\\x9fen den gleichen s', 'iche, genie\\xc3\\x9fen den gleichen sozialen schutz. arti', 'gleichen sozialen schutz. artikel 26 1. jeder hat ', 'hutz. artikel 26 1. jeder hat das recht auf bildun', 'jeder hat das recht auf bildung. die bildung ist u', 'auf bildung. die bildung ist unentgeltlich, zum mi', 'dung ist unentgeltlich, zum mindesten der grundsch', 'ch, zum mindesten der grundschulunterricht und die', 'r grundschulunterricht und die grundlegende bildun', 'ht und die grundlegende bildung. der grundschulunt'], ['\\xa1rok na zvl\\xc3\\xa1\\xc5\\xa1tn\\xc3\\xad p\\xc3\\xa9\\xc4\\x8di a pomoc. v\\xc5\\xa1echny d\\xc4\\x9bti', 'p\\xc3\\xa9\\xc4\\x8di a pomoc. v\\xc5\\xa1echny d\\xc4\\x9bti, a\\xc5\\xa5 man\\xc5\\xbeelsk\\xc3\\xa9 ne', 'chny d\\xc4\\x9bti, a\\xc5\\xa5 man\\xc5\\xbeelsk\\xc3\\xa9 nebo neman\\xc5\\xbeelsk\\xc3\\xa9, po', '\\xbeelsk\\xc3\\xa9 nebo neman\\xc5\\xbeelsk\\xc3\\xa9, poz\\xc3\\xadvaj\\xc3\\xad stejn\\xc3\\xa9 soc', 'elsk\\xc3\\xa9, poz\\xc3\\xadvaj\\xc3\\xad stejn\\xc3\\xa9 soci\\xc3\\xa1ln\\xc3\\xad ochrany. \\xc4\\x8cl', 'tejn\\xc3\\xa9 soci\\xc3\\xa1ln\\xc3\\xad ochrany. \\xc4\\x8cl\\xc3\\xa1nek 26 1. ka\\xc5\\xbed\\xc3\\xbd ', 'hrany. \\xc4\\x8cl\\xc3\\xa1nek 26 1. ka\\xc5\\xbed\\xc3\\xbd m\\xc3\\xa1 pr\\xc3\\xa1vo na vzd\\xc4\\x9bl', '. ka\\xc5\\xbed\\xc3\\xbd m\\xc3\\xa1 pr\\xc3\\xa1vo na vzd\\xc4\\x9bl\\xc3\\xa1n\\xc3\\xad. vzd\\xc4\\x9bl\\xc3\\xa1n\\xc3\\xad n', ' na vzd\\xc4\\x9bl\\xc3\\xa1n\\xc3\\xad. vzd\\xc4\\x9bl\\xc3\\xa1n\\xc3\\xad nech\\xc5\\xa5 je bezplatn\\xc3\\xa9,', '\\xc4\\x9bl\\xc3\\xa1n\\xc3\\xad nech\\xc5\\xa5 je bezplatn\\xc3\\xa9, alespo\\xc5\\x88 v po\\xc4\\x8d\\xc3\\xa1te'], ['d, sygdom, uarbejdsdygtighed, enkestand, alderdom ', 'ygtighed, enkestand, alderdom eller\\n\\n\\x0candet tab af', ' alderdom eller\\n\\n\\x0candet tab af fortjenstmulighed u', 'det tab af fortjenstmulighed under omst\\xc3\\xa6ndigheder', 'mulighed under omst\\xc3\\xa6ndigheder, der ikke er selvfo', '\\xa6ndigheder, der ikke er selvforskyldt. 2. m\\xc3\\xb8dre o', ' er selvforskyldt. 2. m\\xc3\\xb8dre og b\\xc3\\xb8rn har krav p\\xc3\\xa5', '. m\\xc3\\xb8dre og b\\xc3\\xb8rn har krav p\\xc3\\xa5 s\\xc3\\xa6rlig omsorg og h', 'r krav p\\xc3\\xa5 s\\xc3\\xa6rlig omsorg og hj\\xc3\\xa6lp. alle b\\xc3\\xb8rn sk', 'msorg og hj\\xc3\\xa6lp. alle b\\xc3\\xb8rn skal, hvadenten de er '], ['et wettig, zullen dezelfde sociale bescherming gen', 'zelfde sociale bescherming genieten. artikel 26 1.', 'erming genieten. artikel 26 1. een ieder heeft rec', 'ikel 26 1. een ieder heeft recht op onderwijs; het', ' heeft recht op onderwijs; het onderwijs zal koste', 'rwijs; het onderwijs zal kosteloos zijn, althans w', ' zal kosteloos zijn, althans wat het lager en basi', ' althans wat het lager en basisonderwijs betreft. ', 'er en basisonderwijs betreft. het lager onderwijs ', ' betreft. het lager onderwijs zal verplicht zijn. '], ['\\xce\\xb1\\xce\\xbc\\xce\\xb2\\xce\\xac\\xce\\xbd\\xce\\xbf\\xcf\\x85\\xce\\xbd \\xcf\\x84\\xce\\xb7\\xce\\xbd \\xce\\xaf\\xce\\xb4\\xce\\xb9\\xce\\xb1 \\xce\\xba\\xce\\xbf\\xce\\xb9\\xce\\xbd\\xcf\\x89\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce', '\\xb7\\xce\\xbd \\xce\\xaf\\xce\\xb4\\xce\\xb9\\xce\\xb1 \\xce\\xba\\xce\\xbf\\xce\\xb9\\xce\\xbd\\xcf\\x89\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce\\xae \\xcf\\x80\\xcf\\x81\\xce\\xbf\\xcf\\x83\\xcf\\x84\\xce\\xb1\\xcf\\x83\\xce\\xaf\\xce\\xb1', '\\xbd\\xcf\\x89\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce\\xae \\xcf\\x80\\xcf\\x81\\xce\\xbf\\xcf\\x83\\xcf\\x84\\xce\\xb1\\xcf\\x83\\xce\\xaf\\xce\\xb1. \\xce\\x91\\xce\\xa1\\xce\\x98\\xce\\xa1\\xce\\x9f 26 1. \\xce', '\\xcf\\x84\\xce\\xb1\\xcf\\x83\\xce\\xaf\\xce\\xb1. \\xce\\x91\\xce\\xa1\\xce\\x98\\xce\\xa1\\xce\\x9f 26 1. \\xce\\x9a\\xce\\xb1\\xce\\xb8\\xce\\xad\\xce\\xbd\\xce\\xb1\\xcf\\x82 \\xce\\xad\\xcf\\x87\\xce\\xb5', '\\xce\\x9f 26 1. \\xce\\x9a\\xce\\xb1\\xce\\xb8\\xce\\xad\\xce\\xbd\\xce\\xb1\\xcf\\x82 \\xce\\xad\\xcf\\x87\\xce\\xb5\\xce\\xb9 \\xce\\xb4\\xce\\xb9\\xce\\xba\\xce\\xb1\\xce\\xaf\\xcf\\x89\\xce\\xbc\\xce\\xb1 ', '\\xb1\\xcf\\x82 \\xce\\xad\\xcf\\x87\\xce\\xb5\\xce\\xb9 \\xce\\xb4\\xce\\xb9\\xce\\xba\\xce\\xb1\\xce\\xaf\\xcf\\x89\\xce\\xbc\\xce\\xb1 \\xcf\\x83\\xcf\\x84\\xce\\xb7\\xce\\xbd \\xce\\xb5\\xce\\xba\\xcf\\x80\\xce\\xb1\\xce\\xaf\\xce', '\\xb1\\xce\\xaf\\xcf\\x89\\xce\\xbc\\xce\\xb1 \\xcf\\x83\\xcf\\x84\\xce\\xb7\\xce\\xbd \\xce\\xb5\\xce\\xba\\xcf\\x80\\xce\\xb1\\xce\\xaf\\xce\\xb4\\xce\\xb5\\xcf\\x85\\xcf\\x83\\xce\\xb7. \\xce\\x97 \\xce\\xb5\\xce\\xba\\xcf\\x80', '\\xb5\\xce\\xba\\xcf\\x80\\xce\\xb1\\xce\\xaf\\xce\\xb4\\xce\\xb5\\xcf\\x85\\xcf\\x83\\xce\\xb7. \\xce\\x97 \\xce\\xb5\\xce\\xba\\xcf\\x80\\xce\\xb1\\xce\\xaf\\xce\\xb4\\xce\\xb5\\xcf\\x85\\xcf\\x83\\xce\\xb7 \\xcf\\x80\\xcf\\x81\\xce', ' \\xce\\x97 \\xce\\xb5\\xce\\xba\\xcf\\x80\\xce\\xb1\\xce\\xaf\\xce\\xb4\\xce\\xb5\\xcf\\x85\\xcf\\x83\\xce\\xb7 \\xcf\\x80\\xcf\\x81\\xce\\xad\\xcf\\x80\\xce\\xb5\\xce\\xb9 \\xce\\xbd\\xce\\xb1 \\xcf\\x80\\xce\\xb1\\xcf\\x81\\xce', '\\xcf\\x83\\xce\\xb7 \\xcf\\x80\\xcf\\x81\\xce\\xad\\xcf\\x80\\xce\\xb5\\xce\\xb9 \\xce\\xbd\\xce\\xb1 \\xcf\\x80\\xce\\xb1\\xcf\\x81\\xce\\xad\\xcf\\x87\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9 \\xce\\xb4\\xcf\\x89\\xcf\\x81\\xce\\xb5'], [' k\\xc3\\xadv\\xc3\\xbcl sz\\xc3\\xbcletett, ugyanabban a szoci\\xc3\\xa1lis v\\xc3\\xa9de', ' ugyanabban a szoci\\xc3\\xa1lis v\\xc3\\xa9delemben r\\xc3\\xa9szes\\xc3\\xbcl. 2', '\\xa1lis v\\xc3\\xa9delemben r\\xc3\\xa9szes\\xc3\\xbcl. 26. cikk 1. minden sz', 'szes\\xc3\\xbcl. 26. cikk 1. minden szem\\xc3\\xa9lynek joga van a', ' minden szem\\xc3\\xa9lynek joga van a nevel\\xc3\\xa9shez. a neve', 'joga van a nevel\\xc3\\xa9shez. a nevel\\xc3\\xa9snek, legal\\xc3\\xa1bbis', 'ez. a nevel\\xc3\\xa9snek, legal\\xc3\\xa1bbis az elemi \\xc3\\xa9s alapve', 'egal\\xc3\\xa1bbis az elemi \\xc3\\xa9s alapvet\\xc5\\x91 oktat\\xc3\\xa1st illet\\xc5', '\\xc3\\xa9s alapvet\\xc5\\x91 oktat\\xc3\\xa1st illet\\xc5\\x91en, ingyenesnek kel', '\\xa1st illet\\xc5\\x91en, ingyenesnek kell lennie. az elemi o'], ['di perdita di mezzi di sussistenza per circostanze', 'di sussistenza per circostanze indipendenti dalla ', 'ircostanze indipendenti dalla sua volont\\xc3\\xa0. 2. la ', \"nti dalla sua volont\\xc3\\xa0. 2. la maternit\\xc3\\xa0 e l'infan\", \"\\xc3\\xa0. 2. la maternit\\xc3\\xa0 e l'infanzia hanno diritto a \", \" e l'infanzia hanno diritto a speciali cure ed ass\", 'diritto a speciali cure ed assistenza. tutti i bam', 'ure ed assistenza. tutti i bambini, nati nel matri', 'utti i bambini, nati nel matrimonio o fuori di ess', ' nel matrimonio o fuori di esso, devono godere del'], ['\\xae\\xe5\\x85\\x90 \\xe7\\xab\\xa5\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe5\\xab\\xa1\\xe5\\x87\\xba\\xe3\\x81\\xa7\\xe3\\x81\\x82\\xe3\\x82\\x8b\\xe3\\x81\\xa8\\xe5\\x90\\xa6\\xe3\\x81\\xa8\\xe3\\x82\\x92\\xe5\\x95\\x8f\\xe3\\x82\\x8f\\xe3\\x81\\x9a', '\\xe3\\x81\\xa7\\xe3\\x81\\x82\\xe3\\x82\\x8b\\xe3\\x81\\xa8\\xe5\\x90\\xa6\\xe3\\x81\\xa8\\xe3\\x82\\x92\\xe5\\x95\\x8f\\xe3\\x82\\x8f\\xe3\\x81\\x9a\\xe3\\x80\\x81\\xe5\\x90\\x8c\\xe3\\x81\\x98\\xe7\\xa4\\xbe\\xe4\\xbc\\x9a\\xe7\\x9a\\x84\\xe4\\xbf', '\\x92\\xe5\\x95\\x8f\\xe3\\x82\\x8f\\xe3\\x81\\x9a\\xe3\\x80\\x81\\xe5\\x90\\x8c\\xe3\\x81\\x98\\xe7\\xa4\\xbe\\xe4\\xbc\\x9a\\xe7\\x9a\\x84\\xe4\\xbf\\x9d\\xe8\\xad\\xb7\\xe3\\x82\\x92\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3', '\\xa4\\xbe\\xe4\\xbc\\x9a\\xe7\\x9a\\x84\\xe4\\xbf\\x9d\\xe8\\xad\\xb7\\xe3\\x82\\x92\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82 \\xe7\\xac\\xac26\\xe6\\x9d\\xa1 1. \\xe3\\x81\\x99\\xe3\\x81', '\\xe6\\x9c\\x89\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82 \\xe7\\xac\\xac26\\xe6\\x9d\\xa1 1. \\xe3\\x81\\x99\\xe3\\x81\\xb9\\xe3\\x81\\xa6\\xe4\\xba\\xba\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe6\\x95\\x99\\xe8\\x82\\xb2\\xe3', '\\xa1 1. \\xe3\\x81\\x99\\xe3\\x81\\xb9\\xe3\\x81\\xa6\\xe4\\xba\\xba\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe6\\x95\\x99\\xe8\\x82\\xb2\\xe3\\x82\\x92\\xe5\\x8f\\x97\\xe3\\x81\\x91\\xe3\\x82\\x8b\\xe6\\xa8\\xa9\\xe5\\x88\\xa9\\xe3\\x82\\x92', '\\xe3\\x80\\x81\\xe6\\x95\\x99\\xe8\\x82\\xb2\\xe3\\x82\\x92\\xe5\\x8f\\x97\\xe3\\x81\\x91\\xe3\\x82\\x8b\\xe6\\xa8\\xa9\\xe5\\x88\\xa9\\xe3\\x82\\x92\\xe6\\x9c\\x89\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82\\xe6\\x95\\x99\\xe8\\x82\\xb2\\xe3\\x81', '\\x8b\\xe6\\xa8\\xa9\\xe5\\x88\\xa9\\xe3\\x82\\x92\\xe6\\x9c\\x89\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82\\xe6\\x95\\x99\\xe8\\x82\\xb2\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe5\\xb0\\x91\\xe3\\x81\\xaa\\xe3\\x81\\x8f\\xe3\\x81\\xa8\\xe3\\x82\\x82\\xe5', '\\x80\\x82\\xe6\\x95\\x99\\xe8\\x82\\xb2\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe5\\xb0\\x91\\xe3\\x81\\xaa\\xe3\\x81\\x8f\\xe3\\x81\\xa8\\xe3\\x82\\x82\\xe5\\x88\\x9d\\xe7\\xad\\x89 \\xe3\\x81\\xae\\xe5\\x8f\\x8a\\xe3\\x81\\xb3\\xe5\\x9f\\xba\\xe7\\xa4', '\\xe3\\x81\\x8f\\xe3\\x81\\xa8\\xe3\\x82\\x82\\xe5\\x88\\x9d\\xe7\\xad\\x89 \\xe3\\x81\\xae\\xe5\\x8f\\x8a\\xe3\\x81\\xb3\\xe5\\x9f\\xba\\xe7\\xa4\\x8e\\xe7\\x9a\\x84\\xe3\\x81\\xae\\xe6\\xae\\xb5\\xe9\\x9a\\x8e\\xe3\\x81\\xab\\xe3\\x81\\x8a\\xe3'], ['\\xc2\\xa0l\\xc4\\xabdzek\\xc4\\xbcu\\xc2\\xa0zaud\\xc4\\x93jumu\\xc2\\xa0gad\\xc4\\xabjumos,\\xc2\\xa0kas\\xc2\\xa0radu\\xc5\\xa1', 'jumu\\xc2\\xa0gad\\xc4\\xabjumos,\\xc2\\xa0kas\\xc2\\xa0radu\\xc5\\xa1ies\\xc2\\xa0no\\xc2\\xa0vi\\xc5\\x86a\\xc2\\xa0neat', 'as\\xc2\\xa0radu\\xc5\\xa1ies\\xc2\\xa0no\\xc2\\xa0vi\\xc5\\x86a\\xc2\\xa0neatkar\\xc4\\xabgu\\xc2\\xa0 apst\\xc4\\x81k\\xc4\\xbcu', 'i\\xc5\\x86a\\xc2\\xa0neatkar\\xc4\\xabgu\\xc2\\xa0 apst\\xc4\\x81k\\xc4\\xbcu\\xc2\\xa0d\\xc4\\x93\\xc4\\xbc.\\xc2\\xa0 2.\\xc2\\xa0m\\xc4\\x81t\\xc4', 'apst\\xc4\\x81k\\xc4\\xbcu\\xc2\\xa0d\\xc4\\x93\\xc4\\xbc.\\xc2\\xa0 2.\\xc2\\xa0m\\xc4\\x81t\\xc4\\x93m\\xc2\\xa0un\\xc2\\xa0b\\xc4\\x93rniem\\xc2\\xa0ir', ' 2.\\xc2\\xa0m\\xc4\\x81t\\xc4\\x93m\\xc2\\xa0un\\xc2\\xa0b\\xc4\\x93rniem\\xc2\\xa0ir\\xc2\\xa0ties\\xc4\\xabbas\\xc2\\xa0uz\\xc2\\xa0\\xc4\\xabp', '\\x93rniem\\xc2\\xa0ir\\xc2\\xa0ties\\xc4\\xabbas\\xc2\\xa0uz\\xc2\\xa0\\xc4\\xabpa\\xc5\\xa1u\\xc2\\xa0aizsardz\\xc4\\xabbu\\xc2\\xa0', 's\\xc2\\xa0uz\\xc2\\xa0\\xc4\\xabpa\\xc5\\xa1u\\xc2\\xa0aizsardz\\xc4\\xabbu\\xc2\\xa0un\\xc2\\xa0pal\\xc4\\xabdz\\xc4\\xabbu.\\xc2\\xa0vi', 'ardz\\xc4\\xabbu\\xc2\\xa0un\\xc2\\xa0pal\\xc4\\xabdz\\xc4\\xabbu.\\xc2\\xa0visiem\\xc2\\xa0b\\xc4\\x93rniem,\\xc2\\xa0 la', 'z\\xc4\\xabbu.\\xc2\\xa0visiem\\xc2\\xa0b\\xc4\\x93rniem,\\xc2\\xa0 laul\\xc4\\xabb\\xc4\\x81\\xc2\\xa0un\\xc2\\xa0\\xc4\\x81rlaul'], ['r nesantuokiniai, naudojasi vienoda socialine apsa', 'udojasi vienoda socialine apsauga. 26 straipsnis 1', 'aline apsauga. 26 straipsnis 1. kiekvienas turi te', 'raipsnis 1. kiekvienas turi teis\\xc4\\x99 \\xc4\\xaf moksl\\xc4\\x85. mok', 'as turi teis\\xc4\\x99 \\xc4\\xaf moksl\\xc4\\x85. mokslas, bent jau pradi', 'ksl\\xc4\\x85. mokslas, bent jau pradinio ir pagrindinio i', ' jau pradinio ir pagrindinio i\\xc5\\xa1silavinimo lygmen\\xc5', 'rindinio i\\xc5\\xa1silavinimo lygmen\\xc5\\xb3, yra nemokamas. pr', 'mo lygmen\\xc5\\xb3, yra nemokamas. pradinis mokslas yra p', 'okamas. pradinis mokslas yra privalomas. techninis'], [', senex aut defectus a copiis, invitus. 2. gignend', ' a copiis, invitus. 2. gignendi tempus et puerilis', '2. gignendi tempus et puerilis aetas maxima cura c', 't puerilis aetas maxima cura custodienda sunt. omn', 'ima cura custodienda sunt. omnibus pueris, et matr', ' sunt. omnibus pueris, et matrimonium et extra ide', 's, et matrimonium et extra idem natis, eadem est h', ' extra idem natis, eadem est humanae societatis tu', 'adem est humanae societatis tutela. xxvi 1. suae q', 'ietatis tutela. xxvi 1. suae quisque ipsius doctri'], ['ueri, sive in matrmonio sive extra matrimonium nat', 'nio sive extra matrimonium nati sunt, eodem social', 'monium nati sunt, eodem sociali praesidio fruuntur', 'dem sociali praesidio fruuntur. xxvi i. omnis jus ', 'o fruuntur. xxvi i. omnis jus habet educationi. ed', 'omnis jus habet educationi. educatio debet gratuit', 'ationi. educatio debet gratuita esse, quod saltem ', 'et gratuita esse, quod saltem pertinet ad primas p', 'od saltem pertinet ad primas praecipuasque littera', 'd primas praecipuasque litteras. primae litterae s'], [\"stand. d'kanner alleguer, ob se am odder n\\xc3\\xabt am b\", 'guer, ob se am odder n\\xc3\\xabt am bestiednes gebuer gin', ' n\\xc3\\xabt am bestiednes gebuer gin, kommen an de genos', 'gebuer gin, kommen an de genoss vun deem selwechte', 'n de genoss vun deem selwechten zoziale schutz. ar', ' selwechten zoziale schutz. artikel 26 1. all m\\xc3\\xabn', \"schutz. artikel 26 1. all m\\xc3\\xabnsch huet d'recht op \", \". all m\\xc3\\xabnsch huet d'recht op bildung. den unterr\\xc3\", \"'recht op bildung. den unterr\\xc3\\xa9cht muss op d'manns\", \"en unterr\\xc3\\xa9cht muss op d'mannst fir d'elementar- a\"], ['\\xa1, kemm barra minnu, g\\xc4\\xa7andhom igawdu l-istess pro', ' g\\xc4\\xa7andhom igawdu l-istess protezzjoni spe\\xc4\\x8bjali. ', 'istess protezzjoni spe\\xc4\\x8bjali. l-artiklu 26. 1. kul', 'pe\\xc4\\x8bjali. l-artiklu 26. 1. kul\\xc4\\xa7add g\\xc4\\xa7andu l-jedd', '26. 1. kul\\xc4\\xa7add g\\xc4\\xa7andu l-jedd g\\xc4\\xa7all-edukazzjoni.', 'ndu l-jedd g\\xc4\\xa7all-edukazzjoni. l-edukazzjoni g\\xc4\\xa7an', \"ukazzjoni. l-edukazzjoni g\\xc4\\xa7anda tkun b'xejn, g\\xc4\\xa7a\", \"joni g\\xc4\\xa7anda tkun b'xejn, g\\xc4\\xa7allinqas fil-gradi el\", 'xejn, g\\xc4\\xa7allinqas fil-gradi elementari u fondament', 'l-gradi elementari u fondamentali tag\\xc4\\xa7ha. ledukaz'], ['odas as crian\\xc3\\xa7as, nascidas dentro ou fora do matr', 'ascidas dentro ou fora do matrim\\xc3\\xb4nio, gozam da me', 'ra do matrim\\xc3\\xb4nio, gozam da mesma protec\\xc3\\xa7\\xc3\\xa3o soci', 'ozam da mesma protec\\xc3\\xa7\\xc3\\xa3o social.\\n\\nartigo 26\\xc2\\xb0\\n1. ', '\\xc3\\xa7\\xc3\\xa3o social.\\n\\nartigo 26\\xc2\\xb0\\n1. toda a pessoa tem di', 'o 26\\xc2\\xb0\\n1. toda a pessoa tem direito \\xc3\\xa0 educa\\xc3\\xa7\\xc3\\xa3o.', 'soa tem direito \\xc3\\xa0 educa\\xc3\\xa7\\xc3\\xa3o. a educa\\xc3\\xa7\\xc3\\xa3o deve s', 'duca\\xc3\\xa7\\xc3\\xa3o. a educa\\xc3\\xa7\\xc3\\xa3o deve ser gratuita, pelo me', '\\xc3\\xa3o deve ser gratuita, pelo menos a correspondente', 'a, pelo menos a correspondente ao ensino elementar'], ['olen \\xc4\\x8da\\xc4\\x8dipa ba\\xc5\\xa1i ulavno arakibe thaj pomo\\xc5\\xa1tari', 'ulavno arakibe thaj pomo\\xc5\\xa1taripe. sa o \\xc4\\x8dhave, bij', 'pomo\\xc5\\xa1taripe. sa o \\xc4\\x8dhave, bijande ko ni\\xc4\\x8dai ja av', '\\x8dhave, bijande ko ni\\xc4\\x8dai ja avri olestar, d\\xc5\\xbeivdin', '\\xc4\\x8dai ja avri olestar, d\\xc5\\xbeivdinena jekhutno socijal', ', d\\xc5\\xbeivdinena jekhutno socijalno arakiba. d\\xc5\\xbeeno 2', 'no socijalno arakiba. d\\xc5\\xbeeno 26 1. sako isiole \\xc4\\x8da', '. d\\xc5\\xbeeno 26 1. sako isiole \\xc4\\x8da\\xc4\\x8dipa ko sikljobe. o', 'isiole \\xc4\\x8da\\xc4\\x8dipa ko sikljobe. o sikljoibe trubul te', 'ikljobe. o sikljoibe trubul te ovel bipo\\xc4\\x8dimo bare'], ['\\xc3\\xaen afara acesteia, se bucur\\xc4\\x83 aceea\\xc5\\x9fi protec\\xc5\\xa3ie', 'se bucur\\xc4\\x83 aceea\\xc5\\x9fi protec\\xc5\\xa3ie social\\xc4\\x83. articolul', 'protec\\xc5\\xa3ie social\\xc4\\x83. articolul 26 1. orice persoan', ' articolul 26 1. orice persoana are dreptul la \\xc3\\xaen', 'ce persoana are dreptul la \\xc3\\xaenv\\xc4\\x83\\xc5\\xa3\\xc4\\x83tur\\xc4\\x83. inv\\xc4\\x83\\xc5', 'tul la \\xc3\\xaenv\\xc4\\x83\\xc5\\xa3\\xc4\\x83tur\\xc4\\x83. inv\\xc4\\x83\\xc5\\xa3\\xc4\\x83m\\xc3\\xaentul trebuie s', '\\xc4\\x83. inv\\xc4\\x83\\xc5\\xa3\\xc4\\x83m\\xc3\\xaentul trebuie s\\xc4\\x83 fie gratuit, cel ', ' trebuie s\\xc4\\x83 fie gratuit, cel pu\\xc5\\xa3in \\xc3\\xaen ceea ce p', 'tuit, cel pu\\xc5\\xa3in \\xc3\\xaen ceea ce prive\\xc5\\x9fte inv\\xc4\\x83\\xc5\\xa3\\xc4\\x83m\\xc3', ' ceea ce prive\\xc5\\x9fte inv\\xc4\\x83\\xc5\\xa3\\xc4\\x83m\\xc3\\xaentul elementar \\xc5\\x9fi '], ['\\xbd\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd0\\xb4\\xd0\\xb0\\xd1\\x8e\\xd1\\x82 \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0 \\xd0\\xbe\\xd1\\x81\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd0', '\\xd1\\x82 \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0 \\xd0\\xbe\\xd1\\x81\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbf\\xd0\\xb5\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5', '\\xbe\\xd1\\x81\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbf\\xd0\\xb5\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xb8 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbc\\xd0\\xbe\\xd1\\x89\\xd1\\x8c. \\xd0\\x92', '\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xb8 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbc\\xd0\\xbe\\xd1\\x89\\xd1\\x8c. \\xd0\\x92\\xd1\\x81\\xd0\\xb5 \\xd0\\xb4\\xd0\\xb5\\xd1\\x82\\xd0\\xb8, \\xd1\\x80\\xd0\\xbe\\xd0', '\\xd0\\xbe\\xd1\\x89\\xd1\\x8c. \\xd0\\x92\\xd1\\x81\\xd0\\xb5 \\xd0\\xb4\\xd0\\xb5\\xd1\\x82\\xd0\\xb8, \\xd1\\x80\\xd0\\xbe\\xd0\\xb4\\xd0\\xb8\\xd0\\xb2\\xd1\\x88\\xd0\\xb8\\xd0\\xb5\\xd1\\x81\\xd1\\x8f \\xd0\\xb2 \\xd0', '\\x82\\xd0\\xb8, \\xd1\\x80\\xd0\\xbe\\xd0\\xb4\\xd0\\xb8\\xd0\\xb2\\xd1\\x88\\xd0\\xb8\\xd0\\xb5\\xd1\\x81\\xd1\\x8f \\xd0\\xb2 \\xd0\\xb1\\xd1\\x80\\xd0\\xb0\\xd0\\xba\\xd0\\xb5 \\xd0\\xb8\\xd0\\xbb\\xd0\\xb8 \\xd0\\xb2\\xd0', '\\xb5\\xd1\\x81\\xd1\\x8f \\xd0\\xb2 \\xd0\\xb1\\xd1\\x80\\xd0\\xb0\\xd0\\xba\\xd0\\xb5 \\xd0\\xb8\\xd0\\xbb\\xd0\\xb8 \\xd0\\xb2\\xd0\\xbd\\xd0\\xb5 \\xd0\\xb1\\xd1\\x80\\xd0\\xb0\\xd0\\xba\\xd0\\xb0, \\xd0\\xb4\\xd0\\xbe', '\\xd0\\xb8\\xd0\\xbb\\xd0\\xb8 \\xd0\\xb2\\xd0\\xbd\\xd0\\xb5 \\xd0\\xb1\\xd1\\x80\\xd0\\xb0\\xd0\\xba\\xd0\\xb0, \\xd0\\xb4\\xd0\\xbe\\xd0\\xbb\\xd0\\xb6\\xd0\\xbd\\xd1\\x8b \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xb7\\xd0', '\\xd0\\xba\\xd0\\xb0, \\xd0\\xb4\\xd0\\xbe\\xd0\\xbb\\xd0\\xb6\\xd0\\xbd\\xd1\\x8b \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f \\xd0\\xbe\\xd0\\xb4\\xd0\\xb8', '\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f \\xd0\\xbe\\xd0\\xb4\\xd0\\xb8\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb9 \\xd1\\x81\\xd0\\xbe\\xd1'], ['\\xadculo 26 1. toda persona tiene derecho a la educac', 'sona tiene derecho a la educaci\\xc3\\xb3n. la educaci\\xc3\\xb3n ', ' la educaci\\xc3\\xb3n. la educaci\\xc3\\xb3n debe ser gratuita, a', 'ducaci\\xc3\\xb3n debe ser gratuita, al menos en lo concer', 'ratuita, al menos en lo concerniente a la instrucc', ' lo concerniente a la instrucci\\xc3\\xb3n elemental y fun', 'a instrucci\\xc3\\xb3n elemental y fundamental. la instruc', 'ntal y fundamental. la instrucci\\xc3\\xb3n elemental ser\\xc3', 'la instrucci\\xc3\\xb3n elemental ser\\xc3\\xa1 obligatoria. la in', 'ental ser\\xc3\\xa1 obligatoria. la instrucci\\xc3\\xb3n t\\xc3\\xa9cnica '], [\"\\x83\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd0\\xb8 \\xd1\\x81\\xd1\\x96\\xd0\\xbc'\\xd1\\x8e. \\xd0\\x92\\xd0\\xbe\\xd0\\xbd\\xd0\\xb8 \\xd0\\xba\\xd0\\xbe\\xd1\\x80\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd1\\x83\\xd1\\x8e\\xd1\\x82\\xd1\\x8c\", ' \\xd0\\x92\\xd0\\xbe\\xd0\\xbd\\xd0\\xb8 \\xd0\\xba\\xd0\\xbe\\xd1\\x80\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd1\\x83\\xd1\\x8e\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f \\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\xd0', '\\xd1\\x82\\xd1\\x83\\xd1\\x8e\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f \\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\xd0\\xb8\\xd0\\xbc\\xd0\\xb8 \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd0\\xbc\\xd0\\xb8', '\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\xd0\\xb8\\xd0\\xbc\\xd0\\xb8 \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd0\\xbc\\xd0\\xb8 \\xd1\\x89\\xd0\\xbe\\xd0\\xb4\\xd0\\xbe \\xd0\\xbe\\xd0\\xb4\\xd1\\x80\\xd1\\x83\\xd0\\xb6', '\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd0\\xbc\\xd0\\xb8 \\xd1\\x89\\xd0\\xbe\\xd0\\xb4\\xd0\\xbe \\xd0\\xbe\\xd0\\xb4\\xd1\\x80\\xd1\\x83\\xd0\\xb6\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd0\\xbf\\xd1\\x96\\xd0\\xb4 \\xd1\\x87\\xd0\\xb0', '\\xd0\\xbe\\xd0\\xb4\\xd1\\x80\\xd1\\x83\\xd0\\xb6\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd0\\xbf\\xd1\\x96\\xd0\\xb4 \\xd1\\x87\\xd0\\xb0\\xd1\\x81 \\xd1\\x88\\xd0\\xbb\\xd1\\x8e\\xd0\\xb1\\xd1\\x83 \\xd1\\x82\\xd0\\xb0 \\xd0', '\\xbf\\xd1\\x96\\xd0\\xb4 \\xd1\\x87\\xd0\\xb0\\xd1\\x81 \\xd1\\x88\\xd0\\xbb\\xd1\\x8e\\xd0\\xb1\\xd1\\x83 \\xd1\\x82\\xd0\\xb0 \\xd0\\xbf\\xd1\\x96\\xd0\\xb4 \\xd1\\x87\\xd0\\xb0\\xd1\\x81 \\xd0\\xb9\\xd0\\xbe\\xd0\\xb3\\xd0', '\\xb1\\xd1\\x83 \\xd1\\x82\\xd0\\xb0 \\xd0\\xbf\\xd1\\x96\\xd0\\xb4 \\xd1\\x87\\xd0\\xb0\\xd1\\x81 \\xd0\\xb9\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd1\\x96\\xd1\\x80\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd', '\\xd1\\x81 \\xd0\\xb9\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd1\\x96\\xd1\\x80\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f. 2. \\xd0\\xa8\\xd0\\xbb\\xd1\\x8e\\xd0\\xb1 \\xd0\\xbc\\xd0\\xbe', '\\xd1\\x80\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f. 2. \\xd0\\xa8\\xd0\\xbb\\xd1\\x8e\\xd0\\xb1 \\xd0\\xbc\\xd0\\xbe\\xd0\\xb6\\xd0\\xb5 \\xd1\\x83\\xd0\\xba\\xd0\\xbb\\xd0\\xb0\\xd0\\xb4\\xd0\\xb0\\xd1\\x82\\xd0'], [' u napan e ngiyal\\xe2\\x80\\x99 famang ni ketal ko marwel ni ', ' famang ni ketal ko marwel ni bochane liliy, mad-a', 'marwel ni bochane liliy, mad-ad u tabane marwel, k', 'liy, mad-ad u tabane marwel, kanim rok\\xe2\\x80\\x99, kepummo', ' marwel, kanim rok\\xe2\\x80\\x99, kepummon ara pilbizir, nge ', '\\x99, kepummon ara pilbizir, nge kutin bay ban\\xe2\\x80\\x99en n', 'izir, nge kutin bay ban\\xe2\\x80\\x99en ni bagaa fan ngak\\xe2\\x80\\x99 ', 'ban\\xe2\\x80\\x99en ni bagaa fan ngak\\xe2\\x80\\x99 ko ayuw rok\\xe2\\x80\\x99. 2. m', 'n ngak\\xe2\\x80\\x99 ko ayuw rok\\xe2\\x80\\x99. 2. ma chitiningen e biti', 'k\\xe2\\x80\\x99. 2. ma chitiningen e bitir nge bitir e kuba y']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "print(string_test)\n",
    "with open(\"test_alllanguages50.csv\", 'wt') as f:\n",
    "    w = csv.writer(f, dialect='excel')\n",
    "    w.writerows(row for row in string_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, prob, language):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model/model_\"+language+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"weights/model_\"+ language + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    with open(\"prob/output_prob_\" + language + \".csv\", 'wt') as f:\n",
    "        w = csv.writer(f, dialect='excel')\n",
    "        for row in prob:\n",
    "            w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# train the model, output generated text after each iteration\n",
    "def train_model(model, X, y, sentences_all_test, string_all_test , chars, char_indices):\n",
    "    print()\n",
    "    #print('-' * 50)\n",
    "    #print('Iteration', iteration)\n",
    "    a=model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=20, verbose = 0)\n",
    "    #model_eng_json = model.to_json()\n",
    "    #with open(\"model/model_eng_\"+str(iteration)+\".json\", \"w\") as json_file:\n",
    "    #    json_file.write(model_eng_json)\n",
    "    # serialize weights to HDF5\n",
    "    #model.save_weights(\"weights/model_eng_\"+ str(iteration) +\".h5\")\n",
    "    #print(\"Saved model to disk\")\n",
    "\n",
    "    #sentence=\n",
    "    prob = []\n",
    "    for idx in range(len(sentences_all_test)):\n",
    "        sentences_test = sentences_all_test[idx]\n",
    "        string_test = string_all_test[idx]\n",
    "        for i in range(len(sentences_test)):\n",
    "            prob.append(get_scores_for_model(sentences_test[i], string_test[i], model,chars, char_indices))\n",
    "    return model, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshabindal/anaconda/envs/env/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model1, prob_eng = train_model(model1, X_eng, y_eng, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model1, prob_eng, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model2, prob_fr = train_model(model2, X_fr, y_fr, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model2, prob_fr, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshabindal/anaconda/envs/env/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model3, prob_ger = train_model(model3, X_ger, y_ger, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model3, prob_ger, 'ger')\n",
    "model4, prob_czc = train_model(model4, X_czc, y_czc, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model4, prob_czc, 'czc')\n",
    "model5, prob_dns = train_model(model5, X_dns, y_dns, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model5, prob_dns, 'dns')\n",
    "model6, prob_dut = train_model(model6, X_dut, y_dut, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model6, prob_dut, 'dut')\n",
    "model7, prob_grk = train_model(model7, X_grk, y_grk, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model7, prob_grk, 'grk')\n",
    "model8, prob_hng = train_model(model8, X_hng, y_hng, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model8, prob_hng, 'hng')\n",
    "\n",
    "model9, prob_itn = train_model(model9, X_itn, y_itn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model9, prob_itn, 'itn')\n",
    "model10, prob_jpn = train_model(model10, X_jpn, y_jpn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model10, prob_jpn, 'jpn')\n",
    "model11, prob_lat = train_model(model11, X_lat, y_lat, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model11, prob_lat, 'lat')\n",
    "model12, prob_lit = train_model(model12, X_lit, y_lit, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model12, prob_lit, 'lit')\n",
    "model13, prob_ltn = train_model(model13, X_ltn, y_ltn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model13, prob_ltn, 'ltn')\n",
    "model14, prob_ltn1 = train_model(model14, X_ltn1, y_ltn1, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model14, prob_ltn1, 'ltn1')\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "model15, prob_lux = train_model(model15, X_lux, y_lux, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model15, prob_lux, 'lux')\n",
    "model16, prob_mls = train_model(model16, X_mls, y_mls, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model16, prob_mls, 'mls')\n",
    "model17, prob_por = train_model(model17, X_por, y_por, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model17, prob_por, 'por')\n",
    "model18, prob_rmn1 = train_model(model18, X_rmn1, y_rmn1, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model18, prob_rmn1, 'rmn1')\n",
    "\n",
    "model19, prob_rum = train_model(model19, X_rum, y_rum, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model19, prob_rum, 'rum')\n",
    "model20, prob_rus = train_model(model20, X_rus, y_rus, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model20, prob_rus, 'rus')\n",
    "model21, prob_spn = train_model(model21, X_spn, y_spn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model21, prob_spn, 'spn')\n",
    "model22, prob_ukr = train_model(model22, X_ukr, y_ukr, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model22, prob_ukr, 'ukr')\n",
    "model23, prob_yps = train_model(model23, X_yps, y_yps, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model23, prob_yps, 'yps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_language(prob_eng, prob_fr, prob_ger, prob_czc, prob_dns, prob_dut, prob_grk, prob_hng, prob_itn,\n",
    "                    prob_jpn, prob_lat, prob_lit, prob_ltn, prob_ltn1, prob_lux, prob_mls, prob_por,\n",
    "                    prob_rmn1, prob_rum, prob_rus, prob_spn, prob_ukr, prob_yps):\n",
    "    y = [0] * len(prob_eng)\n",
    "    probs_eng = []\n",
    "    probs_frn = []\n",
    "    probs_ger = []\n",
    "    for i in range(len(prob_eng)):\n",
    "        sum_eng= sum_fr= sum_ger= sum_czc= sum_dns= sum_dut= sum_grk= sum_hng= sum_itn=sum_jpn= sum_lat= sum_lit= sum_ltn= sum_ltn1= sum_lux= sum_mls= sum_por= sum_rmn1=sum_rum= sum_rus= sum_spn= sum_ukr= sum_yps = 0\n",
    "        for j in range(len(prob_eng[0])):\n",
    "            sum_eng += float(prob_eng[i][j])\n",
    "            sum_fr += float(prob_fr[i][j])\n",
    "            sum_ger += float(prob_ger[i][j])\n",
    "            sum_czc += float(prob_czc[i][j])\n",
    "            sum_dns += float(prob_dns[i][j])\n",
    "            sum_dut += float(prob_dut[i][j])\n",
    "            sum_grk += float(prob_grk[i][j])\n",
    "\n",
    "            sum_hng += float(prob_hng[i][j])\n",
    "            sum_itn += float(prob_itn[i][j])\n",
    "            sum_jpn += float(prob_jpn[i][j])\n",
    "            sum_lat += float(prob_lat[i][j])\n",
    "            sum_lit += float(prob_lit[i][j])\n",
    "            sum_ltn += float(prob_ltn[i][j])\n",
    "\n",
    "            sum_ltn1 += float(prob_ltn1[i][j])\n",
    "            sum_lux += float(prob_lux[i][j])\n",
    "            sum_por += float(prob_por[i][j])\n",
    "            sum_rmn1 += float(prob_rmn1[i][j])\n",
    "            sum_rum += float(prob_rum[i][j])\n",
    "            sum_rus += float(prob_rus[i][j])\n",
    "\n",
    "            sum_spn += float(prob_spn[i][j])\n",
    "            sum_ukr += float(prob_ukr[i][j])\n",
    "            sum_yps += float(prob_yps[i][j])\n",
    "\n",
    "        sum_eng = np.exp(sum_eng)\n",
    "        sum_fr = np.exp(sum_fr)\n",
    "        sum_ger = np.exp(sum_ger)\n",
    "        sum_czc = np.exp(sum_czc)\n",
    "        sum_dns = np.exp(sum_dns)\n",
    "        sum_dut = np.exp(sum_dut)\n",
    "        sum_grk = np.exp(sum_grk)\n",
    "        sum_hng = np.exp(sum_hng)\n",
    "        sum_itn = np.exp(sum_itn)\n",
    "        sum_jpn = np.exp(sum_jpn)\n",
    "        sum_lat = np.exp(sum_lat)\n",
    "        sum_lit = np.exp(sum_lit)\n",
    "\n",
    "        sum_ltn = np.exp(sum_ltn)\n",
    "        sum_ltn1 = np.exp(sum_ltn1)\n",
    "        sum_lux = np.exp(sum_lux)\n",
    "        sum_por = np.exp(sum_por)\n",
    "        sum_rmn1 = np.exp(sum_rmn1)\n",
    "        sum_rum = np.exp(sum_rum)\n",
    "        sum_rus = np.exp(sum_rus)\n",
    "        sum_spn = np.exp(sum_spn)\n",
    "        sum_ukr = np.exp(sum_ukr)\n",
    "        sum_yps = np.exp(sum_yps)\n",
    "        sum_total = np.array([sum_eng, sum_fr, sum_ger, sum_czc, sum_dns, sum_dut, sum_grk, sum_hng, sum_itn,\n",
    "                    sum_jpn, sum_lat, sum_lit, sum_ltn, sum_ltn1, sum_lux, sum_mls, sum_por, sum_rmn1,\n",
    "                    sum_rum, sum_rus, sum_spn, sum_ukr, sum_yps])\n",
    "        #print(sum_total)\n",
    "        max_idx = np.argmax(sum_total)\n",
    "        y[i] = max_idx\n",
    "        #if sum_fr == 0.0:\n",
    "            #sum_fr = 0.0000000001\n",
    "            #print(str(i) + \"Zero\")\n",
    "        probs_eng.append(sum_eng)\n",
    "        probs_frn.append(sum_fr)\n",
    "        probs_ger.append(sum_ger)\n",
    "    return y, probs_eng, probs_frn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "22 1\n"
     ]
    }
   ],
   "source": [
    "b= e =1\n",
    "c =2\n",
    "d =-1\n",
    "a = np.array([b, c , d])\n",
    "print(a.shape)\n",
    "m = np.argmax(a)\n",
    "print(i,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.31740494607e-57 0.89722283182\n",
      "[[ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0]\n",
      " [ 3  0  0  0  2  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  2  0]\n",
      " [ 0  0  5  1  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  2  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  5  0  1  0  0  0  0  0  0  1  0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  9  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 1  1  0  0  0  0  0  0  0  0  0  0  4  1  0  0  2  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  0  0  0  0  0  0  1  4  0  0  0  0  1  0  2  0  0]\n",
      " [ 0  0  1  0  0  1  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1  0  1  3  1  0  0  0  0  2  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  6  0  0  0  1  1  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  1  0  0  1  0  0  0  0  0  0  0  0  0  5  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  8]]\n",
      "0.647826086957\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"prob/output_prob_\" + 'eng' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_eng = list(w)\n",
    "with open(\"prob/output_prob_\" + 'fr' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_fr = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ger' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ger = list(w)\n",
    "with open(\"prob/output_prob_\" + 'czc' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_czc = list(w)\n",
    "with open(\"prob/output_prob_\" + 'dns' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_dns = list(w)\n",
    "with open(\"prob/output_prob_\" + 'dut' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_dut = list(w)\n",
    "with open(\"prob/output_prob_\" + 'grk' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_grk = list(w)\n",
    "with open(\"prob/output_prob_\" + 'hng' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_hng = list(w)\n",
    "with open(\"prob/output_prob_\" + 'itn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_itn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'jpn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_jpn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'lat' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_lat = list(w)\n",
    "with open(\"prob/output_prob_\" + 'lit' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_lit = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ltn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ltn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ltn1' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ltn1 = list(w)\n",
    "with open(\"prob/output_prob_\" + 'lux' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_lux = list(w)\n",
    "with open(\"prob/output_prob_\" + 'mls' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_mls = list(w)\n",
    "with open(\"prob/output_prob_\" + 'por' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_por = list(w)\n",
    "with open(\"prob/output_prob_\" + 'rmn1' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_rmn1 = list(w)\n",
    "with open(\"prob/output_prob_\" + 'rum' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_rum = list(w)\n",
    "with open(\"prob/output_prob_\" + 'rus' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_rus = list(w)\n",
    "with open(\"prob/output_prob_\" + 'spn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_spn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ukr' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ukr = list(w)\n",
    "with open(\"prob/output_prob_\" + 'yps' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_yps = list(w)\n",
    "#print((prob_english), len(prob_fr))\n",
    "#true_labels = ([0] * 230)# + ([1] * 220)\n",
    "true_labels=[]\n",
    "for i in range(23):\n",
    "    true_labels+=[i]*10\n",
    "#print(true_labels.shape)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, auc, roc_curve\n",
    "#try:\n",
    "pred_y, logprob_english, logprob_fr = predict_language(prob_eng, prob_fr, prob_ger, prob_czc, prob_dns, prob_dut, \n",
    "                                                       prob_grk, prob_hng, prob_itn,\n",
    "                    prob_jpn, prob_lat, prob_lit, prob_ltn, prob_ltn1, prob_lux, prob_mls, prob_por,\n",
    "                    prob_rmn1, prob_rum, prob_rus, prob_spn, prob_ukr, prob_yps)\n",
    "#except:\n",
    "#    continue\n",
    "#x = [e for i, e in enumerate(logprob_fr) if e == 0]\n",
    "#print(x)\n",
    "print(min(logprob_fr), max(logprob_fr))\n",
    "y_hat = np.array([a/b for (a, b) in zip(logprob_english, logprob_fr)])\n",
    "#roc_auc_score(true_labels, y_hat)\n",
    "\n",
    "#print(pred_y)\n",
    "print (confusion_matrix(true_labels, pred_y))\n",
    "print (accuracy_score(true_labels, pred_y))\n",
    "#print (y_hat)\n",
    "try:\n",
    "    fpr, tpr, _ = roc_curve(true_labels, y_hat)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
