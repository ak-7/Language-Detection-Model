{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/akankshabindal/anaconda/envs/env/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"     \\nwith open('subset1/eng.txt','rt') as f:\\n    text1 = f.read().lower()\\nwith open('subset1/frn.txt','rt') as f:\\n    text2 = f.read().lower()\\nwith open('subset/ger.txt','rt') as f:\\n    text3 = f.read().lower()\\nprint('corpus length:', len(text1))\\nprint('corpus length:', len(text2))\\nprint('corpus length:', len(text3))\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen=40\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "text = []\n",
    "for i,l in enumerate(languages):\n",
    "    with open('subset/'+ l + '.txt','rt') as f:\n",
    "        doc = f.read().lower()\n",
    "    text.append(doc)\n",
    "'''     \n",
    "with open('subset1/eng.txt','rt') as f:\n",
    "    text1 = f.read().lower()\n",
    "with open('subset1/frn.txt','rt') as f:\n",
    "    text2 = f.read().lower()\n",
    "with open('subset/ger.txt','rt') as f:\n",
    "    text3 = f.read().lower()\n",
    "print('corpus length:', len(text1))\n",
    "print('corpus length:', len(text2))\n",
    "print('corpus length:', len(text3))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 131\n"
     ]
    }
   ],
   "source": [
    "text1 = text[0]\n",
    "text2 = text[1]\n",
    "text3 = text[2]\n",
    "text4 = text[3]\n",
    "text5 = text[4]\n",
    "\n",
    "text6 = text[5]\n",
    "text7 = text[6]\n",
    "text8 = text[7]\n",
    "text9 = text[8]\n",
    "text10 = text[9]\n",
    "text11 = text[10]\n",
    "text12 = text[11]\n",
    "text13 = text[12]\n",
    "text14 = text[13]\n",
    "text15 = text[14]\n",
    "text16 = text[15]\n",
    "text17 = text[16]\n",
    "text18 = text[17]\n",
    "text19 = text[18]\n",
    "text20 = text[19]\n",
    "text21 = text[20]\n",
    "text22 = text[21]\n",
    "text23 = text[22]\n",
    "chars = sorted(list(set(text[0] + text[1] +text[2] +text[3]+ text[4] + text[5] +text[6] + text[7]+ text[8] + text[9] \n",
    "                        +text[10] +text[11] + text[12] +text[13] +text[14] + text[15] +text[16] +text[17] + text[18] \n",
    "                        +text[19] +text[20] + text[21] + text[22])))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#generating language model\n",
    "def build_model(text, chars, char_indices):\n",
    "    ##training set prepare\n",
    "    maxlen= 40\n",
    "    step = 1\n",
    "    sentences_tr = []\n",
    "    next_chars_tr = []\n",
    "    for i in range(0, int(0.8*len(text)) - maxlen, step):\n",
    "        sentences_tr.append(text[i: i + maxlen])\n",
    "        next_chars_tr.append(text[i + maxlen])\n",
    "\n",
    "    print('nb sequences:', len(sentences_tr))\n",
    "    ##test set prepare\n",
    "    sentences_test = []\n",
    "    string_test= []\n",
    "    step_test=20\n",
    "    for i in range(int(0.8*len(text)) - maxlen,len(text) - maxlen, step_test):\n",
    "        sentences_test.append(text[i: i + maxlen])\n",
    "        string_test.append(text[i+maxlen:i+maxlen+5])\n",
    "    print('nb sequences:', len(sentences_test))\n",
    "\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(sentences_tr), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences_tr), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences_tr):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars_tr[i]]] = 1\n",
    "\n",
    "\n",
    "    # build the model: a single LSTM\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return X, y, sentences_test, string_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 8556\n",
      "nb sequences: 108\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9857\n",
      "nb sequences: 124\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9760\n",
      "nb sequences: 123\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8936\n",
      "nb sequences: 113\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9850\n",
      "nb sequences: 124\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10270\n",
      "nb sequences: 129\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 18195\n",
      "nb sequences: 228\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10589\n",
      "nb sequences: 133\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10244\n",
      "nb sequences: 129\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9935\n",
      "nb sequences: 125\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10852\n",
      "nb sequences: 137\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8808\n",
      "nb sequences: 111\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8012\n",
      "nb sequences: 101\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8056\n",
      "nb sequences: 102\n",
      "Vectorization...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "X_eng, y_eng, sentences_eng_test, string_eng_test, model1 = build_model(text1, chars, char_indices) \n",
    "X_fr, y_fr, sentences_fr_test, string_fr_test, model2 = build_model(text2, chars, char_indices) \n",
    "X_ger, y_ger, sentences_ger_test, string_ger_test, model3 = build_model(text3, chars, char_indices) \n",
    "X_czc, y_czc, sentences_czc_test, string_czc_test, model4 = build_model(text4, chars, char_indices) \n",
    "X_dns, y_dns, sentences_dns_test, string_dns_test, model5 = build_model(text5, chars, char_indices) \n",
    "X_dut, y_dut, sentences_dut_test, string_dut_test, model6 = build_model(text6, chars, char_indices) \n",
    "X_grk, y_grk, sentences_grk_test, string_grk_test, model7 = build_model(text7, chars, char_indices) \n",
    "\n",
    "X_hng, y_hng, sentences_hng_test, string_hng_test, model8 = build_model(text8, chars, char_indices) \n",
    "X_itn, y_itn, sentences_itn_test, string_itn_test, model9 = build_model(text9, chars, char_indices) \n",
    "X_jpn, y_jpn, sentences_jpn_test, string_jpn_test, model10 = build_model(text10, chars, char_indices) \n",
    "X_lat, y_lat, sentences_lat_test, string_lat_test, model11 = build_model(text11, chars, char_indices) \n",
    "X_lit, y_lit, sentences_lit_test, string_lit_test, model12 = build_model(text12, chars, char_indices) \n",
    "X_ltn, y_ltn, sentences_ltn_test, string_ltn_test, model13 = build_model(text13, chars, char_indices) \n",
    "X_ltn1, y_ltn1, sentences_ltn1_test, string_ltn1_test, model14 = build_model(text14, chars, char_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 10244\n",
      "nb sequences: 129\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9359\n",
      "nb sequences: 118\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9444\n",
      "nb sequences: 119\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9194\n",
      "nb sequences: 116\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 10176\n",
      "nb sequences: 128\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 17442\n",
      "nb sequences: 219\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 9788\n",
      "nb sequences: 123\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 16456\n",
      "nb sequences: 207\n",
      "Vectorization...\n",
      "Build model...\n",
      "nb sequences: 8850\n",
      "nb sequences: 112\n",
      "Vectorization...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "X_lux, y_lux, sentences_lux_test, string_lux_test, model15 = build_model(text15, chars, char_indices) \n",
    "X_mls, y_mls, sentences_mls_test, string_mls_test, model16 = build_model(text16, chars, char_indices) \n",
    "X_por, y_por, sentences_por_test, string_por_test, model17 = build_model(text17, chars, char_indices) \n",
    "X_rmn1, y_rmn1, sentences_rmn1_test, string_rmn1_test, model18 = build_model(text18, chars, char_indices) \n",
    "X_rum, y_rum, sentences_rum_test, string_rum_test, model19 = build_model(text19, chars, char_indices) \n",
    "X_rus, y_rus, sentences_rus_test, string_rus_test, model20 = build_model(text20, chars, char_indices) \n",
    "X_spn, y_spn, sentences_spn_test, string_spn_test, model21 = build_model(text21, chars, char_indices)\n",
    "\n",
    "X_ukr, y_ukr, sentences_ukr_test, string_ukr_test, model22 = build_model(text22, chars, char_indices) \n",
    "X_yps, y_yps, sentences_yps_test, string_yps_test, model23 = build_model(text23, chars, char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(sentence, chars , char_indices ):\n",
    "    X = np.zeros((1, maxlen, len(chars)), dtype=np.bool)\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[0, t, char_indices[char]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_scores_for_model(seed,next_string, model,chars,char_indices):\n",
    "    sentence =seed\n",
    "    prob_char=0.0\n",
    "    x = np.zeros((5, maxlen, len(chars)))\n",
    "\n",
    "    x[0] = vectorize(seed,chars,char_indices)\n",
    "    x[1] = vectorize(seed[1:]+next_string[:1],chars,char_indices)\n",
    "    x[2] = vectorize(seed[2:]+next_string[:2],chars,char_indices)\n",
    "    x[3] = vectorize(seed[3:]+next_string[:3],chars,char_indices)\n",
    "    x[4] = vectorize(seed[4:]+next_string[:4],chars,char_indices)\n",
    "    probs_term = []\n",
    "    for i in range(5):\n",
    "        preds = model.predict(x[i].reshape(1,40,131), verbose=0)[0]\n",
    "        preds = np.log(preds)\n",
    "        probs_term.append(preds[char_indices[next_string[i]]])\n",
    "\n",
    "    return probs_term\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "#Create a test SET of all language sentences\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "sentences_test=[]\n",
    "string_test=[]\n",
    "test_size = 10\n",
    "sentences_test.append(sentences_fr_test[:test_size])\n",
    "string_test.append(string_fr_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_eng_test[:test_size])\n",
    "string_test.append(string_eng_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_ger_test[:test_size])\n",
    "string_test.append(string_ger_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_czc_test[:test_size])\n",
    "string_test.append(string_czc_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_dns_test[:test_size])\n",
    "string_test.append(string_dns_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_dut_test[:test_size])\n",
    "string_test.append(string_dut_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_grk_test[:test_size])\n",
    "string_test.append(string_grk_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_hng_test[:test_size])\n",
    "string_test.append(string_hng_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_itn_test[:test_size])\n",
    "string_test.append(string_itn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_jpn_test[:test_size])\n",
    "string_test.append(string_jpn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_lat_test[:test_size])\n",
    "string_test.append(string_lat_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_lit_test[:test_size])\n",
    "string_test.append(string_lit_test[:test_size])\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "sentences_test.append(sentences_ltn_test[:test_size])\n",
    "string_test.append(string_ltn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_ltn1_test[:test_size])\n",
    "string_test.append(string_ltn1_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_lux_test[:test_size])\n",
    "string_test.append(string_lux_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_mls_test[:test_size])\n",
    "string_test.append(string_mls_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_por_test[:test_size])\n",
    "string_test.append(string_por_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_rmn1_test[:test_size])\n",
    "string_test.append(string_rmn1_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_rum_test[:test_size])\n",
    "string_test.append(string_rum_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_rus_test[:test_size])\n",
    "string_test.append(string_rus_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_spn_test[:test_size])\n",
    "string_test.append(string_spn_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_ukr_test[:test_size])\n",
    "string_test.append(string_ukr_test[:test_size])\n",
    "\n",
    "sentences_test.append(sentences_yps_test[:test_size])\n",
    "string_test.append(string_yps_test[:test_size])\n",
    "import csv\n",
    "with open(\"test_all.csv\", 'wt') as f:\n",
    "    w = csv.writer(f, dialect='excel')\n",
    "    for row in string_test:\n",
    "        w.writerow(row)\n",
    "print(len(sentences_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, prob, language):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model/model_\"+language+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"weights/model_\"+ language + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    with open(\"prob/output_prob_\" + language + \".csv\", 'wt') as f:\n",
    "        w = csv.writer(f, dialect='excel')\n",
    "        for row in prob:\n",
    "            w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# train the model, output generated text after each iteration\n",
    "def train_model(model, X, y, sentences_all_test, string_all_test , chars, char_indices):\n",
    "    print()\n",
    "    #print('-' * 50)\n",
    "    #print('Iteration', iteration)\n",
    "    a=model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=20, verbose = 0)\n",
    "    #model_eng_json = model.to_json()\n",
    "    #with open(\"model/model_eng_\"+str(iteration)+\".json\", \"w\") as json_file:\n",
    "    #    json_file.write(model_eng_json)\n",
    "    # serialize weights to HDF5\n",
    "    #model.save_weights(\"weights/model_eng_\"+ str(iteration) +\".h5\")\n",
    "    #print(\"Saved model to disk\")\n",
    "\n",
    "    #sentence=\n",
    "    prob = []\n",
    "    for idx in range(len(sentences_all_test)):\n",
    "        sentences_test = sentences_all_test[idx]\n",
    "        string_test = string_all_test[idx]\n",
    "        for i in range(len(sentences_test)):\n",
    "            prob.append(get_scores_for_model(sentences_test[i], string_test[i], model,chars, char_indices))\n",
    "    return model, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshabindal/anaconda/envs/env/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model1, prob_eng = train_model(model1, X_eng, y_eng, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model1, prob_eng, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshabindal/anaconda/envs/env/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'prob/output_prob_fr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8ee4a6aff8de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_fr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_fr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-80573e326c9e>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, prob, language)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model to disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prob/output_prob_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'excel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'prob/output_prob_fr.csv'"
     ]
    }
   ],
   "source": [
    "model2, prob_fr = train_model(model2, X_fr, y_fr, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model2, prob_fr, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshabindal/anaconda/envs/env/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n",
      "\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model3, prob_ger = train_model(model3, X_ger, y_ger, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model3, prob_ger, 'ger')\n",
    "model4, prob_czc = train_model(model4, X_czc, y_czc, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model4, prob_czc, 'czc')\n",
    "model5, prob_dns = train_model(model5, X_dns, y_dns, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model5, prob_dns, 'dns')\n",
    "model6, prob_dut = train_model(model6, X_dut, y_dut, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model6, prob_dut, 'dut')\n",
    "model7, prob_grk = train_model(model7, X_grk, y_grk, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model7, prob_grk, 'grk')\n",
    "model8, prob_hng = train_model(model8, X_hng, y_hng, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model8, prob_hng, 'hng')\n",
    "\n",
    "model9, prob_itn = train_model(model9, X_itn, y_itn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model9, prob_itn, 'itn')\n",
    "model10, prob_jpn = train_model(model10, X_jpn, y_jpn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model10, prob_jpn, 'jpn')\n",
    "model11, prob_lat = train_model(model11, X_lat, y_lat, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model11, prob_lat, 'lat')\n",
    "model12, prob_lit = train_model(model12, X_lit, y_lit, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model12, prob_lit, 'lit')\n",
    "model13, prob_ltn = train_model(model13, X_ltn, y_ltn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model13, prob_ltn, 'ltn')\n",
    "model14, prob_ltn1 = train_model(model14, X_ltn1, y_ltn1, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model14, prob_ltn1, 'ltn1')\n",
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']\n",
    "model15, prob_lux = train_model(model15, X_lux, y_lux, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model15, prob_lux, 'lux')\n",
    "model16, prob_mls = train_model(model16, X_mls, y_mls, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model16, prob_mls, 'mls')\n",
    "model17, prob_por = train_model(model17, X_por, y_por, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model17, prob_por, 'por')\n",
    "model18, prob_rmn1 = train_model(model18, X_rmn1, y_rmn1, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model18, prob_rmn1, 'rmn1')\n",
    "\n",
    "model19, prob_rum = train_model(model19, X_rum, y_rum, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model19, prob_rum, 'rum')\n",
    "model20, prob_rus = train_model(model20, X_rus, y_rus, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model20, prob_rus, 'rus')\n",
    "model21, prob_spn = train_model(model21, X_spn, y_spn, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model21, prob_spn, 'spn')\n",
    "model22, prob_ukr = train_model(model22, X_ukr, y_ukr, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model22, prob_ukr, 'ukr')\n",
    "model23, prob_yps = train_model(model23, X_yps, y_yps, sentences_test, string_test, chars, char_indices)\n",
    "save_model(model23, prob_yps, 'yps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_language(prob_eng, prob_fr, prob_ger, prob_czc, prob_dns, prob_dut, prob_grk, prob_hng, prob_itn,\n",
    "                    prob_jpn, prob_lat, prob_lit, prob_ltn, prob_ltn1, prob_lux, prob_mls, prob_por,\n",
    "                    prob_rmn1, prob_rum, prob_rus, prob_spn, prob_ukr, prob_yps):\n",
    "    y = [0] * len(prob_eng)\n",
    "    probs_eng = []\n",
    "    probs_frn = []\n",
    "    probs_ger = []\n",
    "    for i in range(len(prob_eng)):\n",
    "        sum_eng= sum_fr= sum_ger= sum_czc= sum_dns= sum_dut= sum_grk= sum_hng= sum_itn=sum_jpn= sum_lat= sum_lit= sum_ltn= sum_ltn1= sum_lux= sum_mls= sum_por= sum_rmn1=sum_rum= sum_rus= sum_spn= sum_ukr= sum_yps = 0\n",
    "        for j in range(len(prob_eng[0])):\n",
    "            sum_eng += float(prob_eng[i][j])\n",
    "            sum_fr += float(prob_fr[i][j])\n",
    "            sum_ger += float(prob_ger[i][j])\n",
    "            sum_czc += float(prob_czc[i][j])\n",
    "            sum_dns += float(prob_dns[i][j])\n",
    "            sum_dut += float(prob_dut[i][j])\n",
    "            sum_grk += float(prob_grk[i][j])\n",
    "\n",
    "            sum_hng += float(prob_hng[i][j])\n",
    "            sum_itn += float(prob_itn[i][j])\n",
    "            sum_jpn += float(prob_jpn[i][j])\n",
    "            sum_lat += float(prob_lat[i][j])\n",
    "            sum_lit += float(prob_lit[i][j])\n",
    "            sum_ltn += float(prob_ltn[i][j])\n",
    "\n",
    "            sum_ltn1 += float(prob_ltn1[i][j])\n",
    "            sum_lux += float(prob_lux[i][j])\n",
    "            sum_por += float(prob_por[i][j])\n",
    "            sum_rmn1 += float(prob_rmn1[i][j])\n",
    "            sum_rum += float(prob_rum[i][j])\n",
    "            sum_rus += float(prob_rus[i][j])\n",
    "\n",
    "            sum_spn += float(prob_spn[i][j])\n",
    "            sum_ukr += float(prob_ukr[i][j])\n",
    "            sum_yps += float(prob_yps[i][j])\n",
    "\n",
    "        sum_eng = np.exp(sum_eng)\n",
    "        sum_fr = np.exp(sum_fr)\n",
    "        sum_ger = np.exp(sum_ger)\n",
    "        sum_czc = np.exp(sum_czc)\n",
    "        sum_dns = np.exp(sum_dns)\n",
    "        sum_dut = np.exp(sum_dut)\n",
    "        sum_grk = np.exp(sum_grk)\n",
    "        sum_hng = np.exp(sum_hng)\n",
    "        sum_itn = np.exp(sum_itn)\n",
    "        sum_jpn = np.exp(sum_jpn)\n",
    "        sum_lat = np.exp(sum_lat)\n",
    "        sum_lit = np.exp(sum_lit)\n",
    "\n",
    "        sum_ltn = np.exp(sum_ltn)\n",
    "        sum_ltn1 = np.exp(sum_ltn1)\n",
    "        sum_lux = np.exp(sum_lux)\n",
    "        sum_por = np.exp(sum_por)\n",
    "        sum_rmn1 = np.exp(sum_rmn1)\n",
    "        sum_rum = np.exp(sum_rum)\n",
    "        sum_rus = np.exp(sum_rus)\n",
    "        sum_spn = np.exp(sum_spn)\n",
    "        sum_ukr = np.exp(sum_ukr)\n",
    "        sum_yps = np.exp(sum_yps)\n",
    "        sum_total = np.array([sum_eng, sum_fr, sum_ger, sum_czc, sum_dns, sum_dut, sum_grk, sum_hng, sum_itn,\n",
    "                    sum_jpn, sum_lat, sum_lit, sum_ltn, sum_ltn1, sum_lux, sum_mls, sum_por, sum_rmn1,\n",
    "                    sum_rum, sum_rus, sum_spn, sum_ukr, sum_yps])\n",
    "        #print(sum_total)\n",
    "        max_idx = np.argmax(sum_total)\n",
    "        y[i] = max_idx\n",
    "        #if sum_fr == 0.0:\n",
    "            #sum_fr = 0.0000000001\n",
    "            #print(str(i) + \"Zero\")\n",
    "        probs_eng.append(sum_eng)\n",
    "        probs_frn.append(sum_fr)\n",
    "        probs_ger.append(sum_ger)\n",
    "    return y, probs_eng, probs_frn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "22 1\n"
     ]
    }
   ],
   "source": [
    "b= e =1\n",
    "c =2\n",
    "d =-1\n",
    "a = np.array([b, c , d])\n",
    "print(a.shape)\n",
    "m = np.argmax(a)\n",
    "print(i,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.31740494607e-57 0.89722283182\n",
      "[[ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0]\n",
      " [ 3  0  0  0  2  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  2  0]\n",
      " [ 0  0  5  1  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  2  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  5  0  1  0  0  0  0  0  0  1  0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  9  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 1  1  0  0  0  0  0  0  0  0  0  0  4  1  0  0  2  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  0  0  0  0  0  0  1  4  0  0  0  0  1  0  2  0  0]\n",
      " [ 0  0  1  0  0  1  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1  0  1  3  1  0  0  0  0  2  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  6  0  0  0  1  1  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  1  0  0  1  0  0  0  0  0  0  0  0  0  5  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  8]]\n",
      "0.647826086957\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"prob/output_prob_\" + 'eng' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_eng = list(w)\n",
    "with open(\"prob/output_prob_\" + 'fr' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_fr = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ger' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ger = list(w)\n",
    "with open(\"prob/output_prob_\" + 'czc' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_czc = list(w)\n",
    "with open(\"prob/output_prob_\" + 'dns' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_dns = list(w)\n",
    "with open(\"prob/output_prob_\" + 'dut' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_dut = list(w)\n",
    "with open(\"prob/output_prob_\" + 'grk' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_grk = list(w)\n",
    "with open(\"prob/output_prob_\" + 'hng' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_hng = list(w)\n",
    "with open(\"prob/output_prob_\" + 'itn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_itn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'jpn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_jpn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'lat' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_lat = list(w)\n",
    "with open(\"prob/output_prob_\" + 'lit' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_lit = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ltn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ltn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ltn1' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ltn1 = list(w)\n",
    "with open(\"prob/output_prob_\" + 'lux' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_lux = list(w)\n",
    "with open(\"prob/output_prob_\" + 'mls' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_mls = list(w)\n",
    "with open(\"prob/output_prob_\" + 'por' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_por = list(w)\n",
    "with open(\"prob/output_prob_\" + 'rmn1' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_rmn1 = list(w)\n",
    "with open(\"prob/output_prob_\" + 'rum' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_rum = list(w)\n",
    "with open(\"prob/output_prob_\" + 'rus' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_rus = list(w)\n",
    "with open(\"prob/output_prob_\" + 'spn' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_spn = list(w)\n",
    "with open(\"prob/output_prob_\" + 'ukr' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_ukr = list(w)\n",
    "with open(\"prob/output_prob_\" + 'yps' + \".csv\", 'rt') as f:\n",
    "        w = csv.reader(f, delimiter=',')\n",
    "        prob_yps = list(w)\n",
    "#print((prob_english), len(prob_fr))\n",
    "#true_labels = ([0] * 230)# + ([1] * 220)\n",
    "true_labels=[]\n",
    "for i in range(23):\n",
    "    true_labels+=[i]*10\n",
    "#print(true_labels.shape)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, auc, roc_curve\n",
    "#try:\n",
    "pred_y, logprob_english, logprob_fr = predict_language(prob_eng, prob_fr, prob_ger, prob_czc, prob_dns, prob_dut, \n",
    "                                                       prob_grk, prob_hng, prob_itn,\n",
    "                    prob_jpn, prob_lat, prob_lit, prob_ltn, prob_ltn1, prob_lux, prob_mls, prob_por,\n",
    "                    prob_rmn1, prob_rum, prob_rus, prob_spn, prob_ukr, prob_yps)\n",
    "#except:\n",
    "#    continue\n",
    "#x = [e for i, e in enumerate(logprob_fr) if e == 0]\n",
    "#print(x)\n",
    "print(min(logprob_fr), max(logprob_fr))\n",
    "y_hat = np.array([a/b for (a, b) in zip(logprob_english, logprob_fr)])\n",
    "#roc_auc_score(true_labels, y_hat)\n",
    "\n",
    "#print(pred_y)\n",
    "print (confusion_matrix(true_labels, pred_y))\n",
    "print (accuracy_score(true_labels, pred_y))\n",
    "#print (y_hat)\n",
    "try:\n",
    "    fpr, tpr, _ = roc_curve(true_labels, y_hat)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "languages = ['eng', 'frn', 'ger', 'czc','dns','dut','grk','hng','itn','jpn','lat','lit','ltn','ltn1','lux','mls','por'\n",
    "             ,'rmn1','rum','rus','spn','ukr','yps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
